{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Connecting Google Drive with Colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I19yvkAbnoEU",
        "outputId": "d21cda10-e400-4e85-8ea2-039365f8368f"
      },
      "id": "I19yvkAbnoEU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d3d5fa25-c69c-4751-8989-790a574ae4fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3d5fa25-c69c-4751-8989-790a574ae4fa",
        "outputId": "04255b40-484d-41d8-b788-f31c45ca9742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] All libraries Imported\n"
          ]
        }
      ],
      "source": [
        "# Importing Required Packages\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "import torch\n",
        "print(\"[DEBUG] All libraries Imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672e53b3-a6d0-4fbb-b536-f8322925a07a",
      "metadata": {
        "id": "672e53b3-a6d0-4fbb-b536-f8322925a07a"
      },
      "outputs": [],
      "source": [
        "# Load synthetic data\n",
        "json_file = \"/content/drive/MyDrive/WEBBIES_NOTEBOOKS/PERSONAL_RESEARCH/Assessli_Research/Modified_Data.json\"\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"[DEBUG] JSON Data Loaded\")\n",
        "\n",
        "rows = []\n",
        "for item in data:\n",
        "    profile = item[\"user_profile\"]\n",
        "    text_input = (\n",
        "        f\"summarize with tone={profile['tone_preference']}, \"\n",
        "        f\"length={profile['summary_length_preference']}, \"\n",
        "        f\"focus={profile['focus_area']}: {item['document_text']}\"\n",
        "    )\n",
        "    target = item[\"user_generated_summary\"]\n",
        "    rows.append({\"input_text\": text_input, \"target_text\": target})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"[DEBUG] Dataframe Conversion is Successfull\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18afb970-b3de-4f56-87ed-f9601dc7dae3",
      "metadata": {
        "id": "18afb970-b3de-4f56-87ed-f9601dc7dae3"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "model_name = \"t5-base\" # t5-small\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "print(\"[DEBUG] T5-Base Model with tokenizer Loaded Successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971e7b34-c854-4492-9573-558f9b7adf3b",
      "metadata": {
        "id": "971e7b34-c854-4492-9573-558f9b7adf3b"
      },
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "max_input_length = 1024\n",
        "max_target_length = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3cadf0e-6a3f-445f-8e0d-f2afe91d9bd3",
      "metadata": {
        "id": "b3cadf0e-6a3f-445f-8e0d-f2afe91d9bd3"
      },
      "outputs": [],
      "source": [
        "# The Preprocess Function\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"input_text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"target_text\"],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba3ba34-0657-4610-a39f-fadeb588419e",
      "metadata": {
        "id": "0ba3ba34-0657-4610-a39f-fadeb588419e"
      },
      "outputs": [],
      "source": [
        "# The Tokenized dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63accf3b-7e04-4e89-81eb-31879ff8b568",
      "metadata": {
        "id": "63accf3b-7e04-4e89-81eb-31879ff8b568"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size = 0.2, seed = 42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "342394ac-fa2b-4adb-8f42-c76630460188",
      "metadata": {
        "id": "342394ac-fa2b-4adb-8f42-c76630460188"
      },
      "outputs": [],
      "source": [
        "# # Setting the Trainer object\n",
        "# trainer = transformers.Trainer(\n",
        "#     model = model,\n",
        "#     train_dataset = tokenized_train_dataset,\n",
        "#     args = transformers.TrainingArguments(\n",
        "#         output_dir = \"./BergerFineTunnedModel\", # The directory where the Fine Tunned Model will be stored\n",
        "#         per_device_train_batch_size = 4, # The default is 2\n",
        "#         gradient_accumulation_steps = 2,\n",
        "#         num_train_epochs = 3, # Increase the number of epochs if needed for better training\n",
        "#         learning_rate = 5e-5,\n",
        "#         max_steps = 20, # Use max steps to show the result for those steps\n",
        "#         bf16 = False,\n",
        "#         optim = \"paged_adamw_8bit\",\n",
        "#         logging_dir = \"./log\",\n",
        "#         save_strategy = \"epoch\",\n",
        "#         save_steps = 50,\n",
        "#         logging_steps = 10\n",
        "\n",
        "# ),\n",
        "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False),\n",
        "# )\n",
        "# model.config.use_cache = False\n",
        "# trainer.train()\n",
        "\n",
        "# print(\"Trainer Loaded Successfully\")\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"/content/drive/MyDrive/WEBBIES_NOTEBOOKS/PERSONAL_RESEARCH/Assessli_Research/lbm_model\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate = 3e-4,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    num_train_epochs = 50,\n",
        "    weight_decay = 0.01,\n",
        "    save_total_limit = 2,\n",
        "    logging_dir = \"./log\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 10,\n",
        "    save_steps = 50,\n",
        "    push_to_hub = False\n",
        ")\n",
        "print(\"[DEBUG] Training Arguements are Set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536fe829-512e-4a6b-ae00-ed6e95db82bd",
      "metadata": {
        "id": "536fe829-512e-4a6b-ae00-ed6e95db82bd"
      },
      "outputs": [],
      "source": [
        "# The data collator object\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac693e7-db57-4325-a02e-5e64eaef694a",
      "metadata": {
        "id": "9ac693e7-db57-4325-a02e-5e64eaef694a"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator,\n",
        ")\n",
        "\n",
        "print(\"[DEBUG] Trainer created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30d8839-24b3-4afc-9e4e-11245fee7efa",
      "metadata": {
        "id": "e30d8839-24b3-4afc-9e4e-11245fee7efa"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "trainer.train()\n",
        "print(\"[DEBUG] Into the Training Process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c355eb52-e622-4686-b20d-c059d15f8537",
      "metadata": {
        "id": "c355eb52-e622-4686-b20d-c059d15f8537"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/WEBBIES_NOTEBOOKS/PERSONAL_RESEARCH/Assessli_Research/lbm_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/WEBBIES_NOTEBOOKS/PERSONAL_RESEARCH/Assessli_Research/lbm_model\")\n",
        "print(\"[DEBUG] Saving the Model and Tokenizer\")\n",
        "\n",
        "print(\"Behaviour-conditioned summarization model saved at ./lbm_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a861e12d-1268-4387-9f3b-6322d98efd0b",
      "metadata": {
        "id": "a861e12d-1268-4387-9f3b-6322d98efd0b"
      },
      "outputs": [],
      "source": [
        "# Example inference\n",
        "def generate_summary(doc_text, tone, length, focus):\n",
        "    input_text = f\"summarize with tone = {tone}, length = {length}, focus = {focus}: {doc_text}\"\n",
        "    input_ids = tokenizer(input_text, return_tensors = \"pt\", truncation = True, max_length = 1024).input_ids\n",
        "    outputs = model.generate(input_ids, max_length = 256, num_beams = 4, early_stopping = True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0e26e0-b714-4e78-9107-40a19d040c6d",
      "metadata": {
        "id": "bf0e26e0-b714-4e78-9107-40a19d040c6d"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "test_doc = \"Artificial Intelligence is transforming healthcare by improving diagnosis and treatment.\"\n",
        "print(generate_summary(test_doc, \"enthusiastic\", \"medium\", \"methods\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}