{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8454eb46-b913-4830-928f-7a5c85212a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.48.1\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Author: \n",
      "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
      "License-Expression: MIT\n",
      "Location: C:\\Users\\Webbies\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, packaging, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36043b8-ed87-403d-bb14-3f82cc8a2847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4e31a3-6898-4c7d-b5ae-6975f90de47a",
   "metadata": {},
   "source": [
    "# CELL - 1: Installing the HF_XET module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ca78ca-0fda-449f-b4de-456f3dcd6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q hf-xet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c773e-7ab0-4f8e-9651-13d93751c820",
   "metadata": {},
   "source": [
    "# CELL - 2: Importing All Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527439e-084b-460c-9921-7396a6cf59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import getpass\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Set the HF Token and Configuration for Future Use\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"your_hf_token\")\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "\n",
    "# helper\n",
    "print(\"HF_TOKEN set:\", bool(HF_TOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe4db6-537e-4220-85d2-b50bbf02f7ae",
   "metadata": {},
   "source": [
    "# CELL - 3: Model Configuration and Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3beb7748-fbc4-4154-94ff-7f18074c3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "MODEL_NAME: meta-llama/Llama-2-7b-chat-hf\n",
      "OUTPUT_DIR: ./lora_personalized_lbm\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # meta-llama/Llama-2-7b-chat-hf or mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "OUTPUT_DIR = \"./lora_personalized_lbm\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok = True)\n",
    "\n",
    "# Training hyperparams\n",
    "BATCH_SIZE = 1   # small for low VRAM; use gradient accumulation to simulate larger B\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.2\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "print(\"Config:\")\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34140f2-8ac4-4587-a8db-8ea9742eb2c6",
   "metadata": {},
   "source": [
    "# CELL - 4: Installing Accelerate Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a21dd4d-74cf-4c0f-aba7-441f973ae335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589ab1a-95c8-4e4d-b986-4fe4d30d1eed",
   "metadata": {},
   "source": [
    "# CELL - 5: Model and Tokenizer Loading in 4 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056d125c-79ac-4aa1-af5b-fe50e197191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model in 4-bit (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c7c4587a2a41eb93e57cb6e67e3c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit = True, llm_int8_enable_fp32_cpu_offload = True)\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast = False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model in 4-bit (this may take a while)...\")\n",
    "\n",
    "# AutoModelForCausalLM supports bitsandbytes integration (depends on model repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\",  # let accelerate choose device placement\n",
    "    trust_remote_code = True,\n",
    "    use_auth_token = HF_TOKEN or None # low_cpu_mem_usage=True  and torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de63885-a7f3-4324-a512-2ff091128430",
   "metadata": {},
   "source": [
    "# CELL - 6: Loading the PENS data from Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0860354e-7555-4406-9b51-328842eca432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique news IDs needed by sampled data: 1572\n",
      "=========================================================\n",
      "The shapes of Respective dataframes are:\n",
      "Train shape (User Logs): (1500, 9)\n",
      "Val shape (User Logs): (300, 9)\n",
      "News shape (Filtered Articles): (786, 7)\n",
      "=========================================================\n",
      "The Column names of Respective dataframes are:\n",
      "Sample Train columns: ['UserID', 'ClicknewsID', 'dwelltime', 'exposure_time', 'pos', 'neg', 'start', 'end', 'dwelltime_pos']\n",
      "Sample Validation columns: ['UserID', 'ClicknewsID', 'dwelltime', 'exposure_time', 'pos', 'neg', 'start', 'end', 'dwelltime_pos']\n",
      "Sample news columns: ['News ID', 'Category', 'Topic', 'Headline', 'News body', 'Title entity', 'Entity content']\n"
     ]
    }
   ],
   "source": [
    "# import os, csv, json\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# DATA_DIR = \"./data/pens\"\n",
    "\n",
    "# train_path = os.path.join(DATA_DIR, \"train.tsv\")\n",
    "# val_path = os.path.join(DATA_DIR, \"valid.tsv\")\n",
    "# news_path = os.path.join(DATA_DIR, \"news.tsv\")\n",
    "\n",
    "# assert os.path.exists(train_path), \"Missing train.tsv\"\n",
    "# assert os.path.exists(val_path), \"Missing validation.tsv\"\n",
    "# assert os.path.exists(news_path), \"Missing news.tsv (required for article text)\"\n",
    "\n",
    "# # Load TSVs\n",
    "# train_df_full = pd.read_csv(train_path, sep = \"\\t\")\n",
    "# val_df_full = pd.read_csv(val_path, sep = \"\\t\")\n",
    "# news_df_full = pd.read_csv(news_path, sep = \"\\t\")\n",
    "\n",
    "\n",
    "# # Sampling the train_df, val_df and news_df with 2000 samples with random state 42\n",
    "# train_df = train_df_full.sample(15000, random_state = 42)\n",
    "# val_df = val_df_full.sample(15000, random_state = 42)\n",
    "# news_df = news_df_full.sample(15000, random_state = 42)\n",
    "\n",
    "# print(\"=========================================================\")\n",
    "\n",
    "# print(\"The shapes of Respective dataframes are:\")\n",
    "# print(\"Train shape:\", train_df.shape)\n",
    "# print(\"Val shape:\", val_df.shape)\n",
    "# print(\"News shape:\", news_df.shape)\n",
    "\n",
    "# print(\"=========================================================\")\n",
    "\n",
    "# print(\"The Column names of Respective dataframes are:\")\n",
    "# print(\"Sample Train columns:\", train_df.columns.tolist())\n",
    "# print(\"Sample Validation columns:\", val_df.columns.tolist())\n",
    "# print(\"Sample news columns:\", news_df.columns.tolist())\n",
    "\n",
    "############################################## NEW MODIFIED CODE ############################################\n",
    "import os, csv, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"./data/pens\"\n",
    "\n",
    "train_path = os.path.join(DATA_DIR, \"train.tsv\")\n",
    "val_path = os.path.join(DATA_DIR, \"valid.tsv\")\n",
    "news_path = os.path.join(DATA_DIR, \"news.tsv\")\n",
    "\n",
    "assert os.path.exists(train_path), \"Missing train.tsv\"\n",
    "assert os.path.exists(val_path), \"Missing validation.tsv\"\n",
    "assert os.path.exists(news_path), \"Missing news.tsv (required for article text)\"\n",
    "\n",
    "# Load TSVs\n",
    "train_df_full = pd.read_csv(train_path, sep = \"\\t\")\n",
    "val_df_full = pd.read_csv(val_path, sep = \"\\t\")\n",
    "news_df_full = pd.read_csv(news_path, sep = \"\\t\")\n",
    "\n",
    "# 1. Sample larger train set and smaller validation set for better ratio\n",
    "TRAIN_SAMPLE_SIZE = 1500  # Increased for higher training sample count\n",
    "VAL_SAMPLE_SIZE = 300    # Decreased for lower validation sample count\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Sample the train and validation logs\n",
    "train_df = train_df_full.sample(TRAIN_SAMPLE_SIZE, random_state = RANDOM_STATE).reset_index(drop=True)\n",
    "val_df = val_df_full.sample(VAL_SAMPLE_SIZE, random_state = RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# 2. Extract ALL unique news IDs required by the sampled train and val sets\n",
    "# The 'pos' column contains semicolon/pipe separated news IDs (which Code-2 parses)\n",
    "def extract_all_news_ids(df):\n",
    "    all_ids = set()\n",
    "    for ids_str in df['pos'].dropna():\n",
    "        # Handle both ';' and '|' separators, and split\n",
    "        ids = str(ids_str).replace(\";\", \"|\").split(\"|\")\n",
    "        all_ids.update([n.strip() for n in ids if n.strip()])\n",
    "    return all_ids\n",
    "\n",
    "required_train_ids = extract_all_news_ids(train_df)\n",
    "required_val_ids = extract_all_news_ids(val_df)\n",
    "required_news_ids = required_train_ids.union(required_val_ids)\n",
    "\n",
    "# 3. Filter the full news_df to only include the articles we actually need\n",
    "# This ensures data linkage is maintained and dramatically increases the hit rate in the build_prompt_target function.\n",
    "news_df = news_df_full[news_df_full['News ID'].isin(required_news_ids)].reset_index(drop = True)\n",
    "\n",
    "print(f\"Total unique news IDs needed by sampled data: {len(required_news_ids)}\")\n",
    "# --- END OF MODIFIED SECTION ---\n",
    "\n",
    "print(\"=========================================================\")\n",
    "\n",
    "print(\"The shapes of Respective dataframes are:\")\n",
    "print(\"Train shape (User Logs):\", train_df.shape)\n",
    "print(\"Val shape (User Logs):\", val_df.shape)\n",
    "print(\"News shape (Filtered Articles):\", news_df.shape) # This size will now be variable\n",
    "print(\"=========================================================\")\n",
    "\n",
    "print(\"The Column names of Respective dataframes are:\")\n",
    "print(\"Sample Train columns:\", train_df.columns.tolist())\n",
    "print(\"Sample Validation columns:\", val_df.columns.tolist())\n",
    "print(\"Sample news columns:\", news_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9fbc9-f648-4465-8f7f-061ab1839e02",
   "metadata": {},
   "source": [
    "# CELL - 7: Merge Data to Build Behaviour-text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fccad69-5ec5-48a4-b239-55c5699e4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 837 training pairs and 176 validation pairs.\n"
     ]
    }
   ],
   "source": [
    "def parse_news_ids(news_ids):\n",
    "    \"\"\"Parse semicolon or pipe separated news IDs into a clean list.\"\"\"\n",
    "    if pd.isna(news_ids):\n",
    "        return []\n",
    "    ids = str(news_ids).replace(\";\", \"|\").split(\"|\")\n",
    "    return [n.strip() for n in ids if n.strip()]\n",
    "\n",
    "\n",
    "def get_user_profile(row, news_df):\n",
    "    \"\"\"\n",
    "    Aggregate user's positive-click history (headlines of clicked articles)\n",
    "    to build behavioral context.\n",
    "    \"\"\"\n",
    "    pos_ids = parse_news_ids(row.get(\"pos\", \"\"))\n",
    "    clicked_headlines = []\n",
    "    for nid in pos_ids[:5]:  # limit to last 5 clicked\n",
    "        if nid in news_df.index:\n",
    "            clicked_headlines.append(str(news_df.loc[nid, \"Headline\"]))\n",
    "    avg_dwell = row.get(\"dwelltime_pos\", 0)\n",
    "    return {\n",
    "        \"user_id\": row[\"UserID\"],\n",
    "        \"clicked_headlines\": clicked_headlines,\n",
    "        \"avg_dwell\": avg_dwell,\n",
    "    }\n",
    "\n",
    "\n",
    "# Set News ID as index for faster lookup\n",
    "news_df = news_df.set_index(\"News ID\")\n",
    "\n",
    "\n",
    "def build_prompt_target(row, news_df):\n",
    "    \"\"\"\n",
    "    Build training samples combining user behavior and article text.\n",
    "    - Input (prompt): combines user profile + article text\n",
    "    - Target: the article's headline (as a personalized summary)\n",
    "    \"\"\"\n",
    "    pos_ids = parse_news_ids(row.get(\"pos\", \"\"))\n",
    "    samples = []\n",
    "\n",
    "    for nid in pos_ids:\n",
    "        if nid in news_df.index:\n",
    "            article = news_df.loc[nid]\n",
    "\n",
    "            # User profile context\n",
    "            profile = get_user_profile(row, news_df)\n",
    "            profile_str = (\n",
    "                f\"<USER:{profile['user_id']}> \"\n",
    "                f\"<AVG_DWELL:{profile['avg_dwell']}> \"\n",
    "                f\"<HISTORY:{'|'.join(profile['clicked_headlines'])}>\"\n",
    "            )\n",
    "\n",
    "            # Use news text and headline\n",
    "            headline = str(article.get(\"Headline\", \"\")).strip()\n",
    "            news_body = str(article.get(\"News body\", \"\")).strip()\n",
    "            topic = str(article.get(\"Topic\", \"\")).strip()\n",
    "            category = str(article.get(\"Category\", \"\")).strip()\n",
    "\n",
    "            doc_text = f\"[Category: {category}] [Topic: {topic}] {news_body}\"\n",
    "\n",
    "            # Headline is the personalized summary target\n",
    "            target = headline\n",
    "\n",
    "            # Prompt\n",
    "            prompt = (\n",
    "                f\"Summarize the following news article for a user with preferences: {profile_str}\\n\\n\"\n",
    "                f\"Article:\\n{doc_text}\\n\\nSummary:\"\n",
    "            )\n",
    "\n",
    "            samples.append({\"prompt\": prompt, \"target\": target})\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Build dataset\n",
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.extend(build_prompt_target(row, news_df))\n",
    "\n",
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.extend(build_prompt_target(row, news_df))\n",
    "\n",
    "print(f\"Generated {len(train_samples)} training pairs and {len(val_samples)} validation pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea83121-83c8-4005-bad4-7226499a8d3e",
   "metadata": {},
   "source": [
    "# CELL - 8: Converting the data into JSONL files for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff80323c-83de-4a09-b491-1baaad28071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to processed_pens/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"processed_pens\", exist_ok = True)\n",
    "\n",
    "with open(\"processed_pens/pens_train.jsonl\", \"w\", encoding = \"utf-8\") as f:\n",
    "    for ex in train_samples:\n",
    "        f.write(json.dumps(ex, ensure_ascii = False) + \"\\n\")\n",
    "        \n",
    "with open(\"processed_pens/pens_val.jsonl\", \"w\", encoding = \"utf-8\") as f:\n",
    "    for ex in val_samples:\n",
    "        f.write(json.dumps(ex, ensure_ascii = False) + \"\\n\")\n",
    "\n",
    "print(\"Saved preprocessed data to processed_pens/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e9d08-402a-4e69-86fb-c216e4455b83",
   "metadata": {},
   "source": [
    "# CELL - 9: Create Dataset class for PENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d6e0933-c1a0-4d94-af72-43316a89b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset Class is created Successfully\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class PENSJsonlDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, tokenizer, max_length: int = 1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(jsonl_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            self.examples = [json.loads(line) for line in f]\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        prompt = ex[\"prompt\"]\n",
    "        target = ex[\"target\"]\n",
    "        text = prompt + \" \" + target\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            padding = \"max_length\",\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze()\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "print(\"The dataset Class is created Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ce290-5118-43a0-8784-84059225312a",
   "metadata": {},
   "source": [
    "# CELL - 10: Initialize training dataset from your processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752ca54e-2b57-4801-8a27-e0bee41b891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready. Training samples: 837\n",
      "Dataset ready. Validation samples: 176\n"
     ]
    }
   ],
   "source": [
    "train_jsonl_path = \"processed_pens/pens_train.jsonl\"\n",
    "val_jsonl_path = \"processed_pens/pens_val.jsonl\"\n",
    "\n",
    "ds = PENSJsonlDataset(train_jsonl_path, tokenizer, max_length = 512)\n",
    "val_ds = PENSJsonlDataset(val_jsonl_path, tokenizer, max_length = 512)\n",
    "\n",
    "print(\"Dataset ready. Training samples:\", len(ds))\n",
    "print(\"Dataset ready. Validation samples:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0703d-68a4-48c8-98a3-7b9c74d65b36",
   "metadata": {},
   "source": [
    "# CELL - 11: Setup LoRA with PEFT and prepare Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "575e582a-a57e-4edb-8543-b32bcbc7e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT LoRA model prepared. Trainable params: 8388608\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "\n",
    "# Prepare model for int8/4bit training (PEFT helper)\n",
    "# Note: prepare_model_for_int8_training supports int8; for 4-bit many users still apply similar prep.\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "except Exception as e:\n",
    "    print(\"prepare_model_for_int8_training hit an issue (ok for 4-bit), continuing:\", e)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_R,\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"] if \"llama\" in MODEL_NAME.lower() else None,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"PEFT LoRA model prepared. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3dab-98e0-4a91-9ccc-4ff5f85a2833",
   "metadata": {},
   "source": [
    "# CELL - 12: Create DataLoader and simple training loop using HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29bdf5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collator Created\n",
      "Training Parameters are set\n",
      "Trainer Created Successfully\n",
      "Starting Hugging Face Trainer fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 1:29:07, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.348700</td>\n",
       "      <td>2.337042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.208900</td>\n",
       "      <td>2.250175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.150300</td>\n",
       "      <td>2.174112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.061400</td>\n",
       "      <td>2.115283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.041100</td>\n",
       "      <td>2.065123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.995200</td>\n",
       "      <td>2.021264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.951900</td>\n",
       "      <td>1.984319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.905900</td>\n",
       "      <td>1.955437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.945000</td>\n",
       "      <td>1.937767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.882700</td>\n",
       "      <td>1.929712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Saved LoRA weights to ./lora_personalized_lbm\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)\n",
    "\n",
    "print(\"Data Collator Created\")\n",
    "\n",
    "# Setting the Training Arguements\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs = EPOCHS,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps = 10,\n",
    "    logging_steps = 10,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 20,\n",
    "    logging_dir = \"./logs\",\n",
    "    report_to = \"none\",\n",
    "    fp16 = True,\n",
    "    save_total_limit = 2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "print(\"Training Parameters are set\")\n",
    "\n",
    "# Creating the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = ds,\n",
    "    eval_dataset = val_ds,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 2)],\n",
    ")\n",
    "print(\"Trainer Created Successfully\")\n",
    "\n",
    "print(\"Starting Hugging Face Trainer fine-tuning...\")\n",
    "\n",
    "# THE ONLY REQUIRED CHANGE\n",
    "# Set 'resume_from_checkpoint=True' to enable automatic resumption.\n",
    "trainer.train() # resume_from_checkpoint = True\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the LoRA adapter weights\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(\"Saved LoRA weights to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad665e-cf33-4c6b-b015-e37c3ce5fe68",
   "metadata": {},
   "source": [
    "# CELL - 13: Inference demo: LoRA + 4-bit generate with conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cd2b004-0e4c-4058-aeb9-793637013c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load base model (same as during training)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model_base \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      7\u001b[0m     MODEL_NAME,\n\u001b[0;32m      8\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# if using 4-bit quantization\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     bnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load LoRA adapter on top of base model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_base, OUTPUT_DIR)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4378\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[0;32m   4379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4380\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[0;32m   4383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1304\u001b[0m, in \u001b[0;36m_get_device_map\u001b[1;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1304\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:104\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Load base model (same as during training)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    "    load_in_4bit = True,  # if using 4-bit quantization\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top of base model\n",
    "model = PeftModel.from_pretrained(model_base, OUTPUT_DIR)\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"The model is loaded in Evaluation Mode with LoRA adapter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate summary\n",
    "def generate_summary(document: str, user_profile: dict, max_new_tokens = 200):\n",
    "    profile_str = f\"<FOCUS:{user_profile['focus']}> <TONE:{user_profile['tone']}> <LENGTH:{user_profile['length']}> <HISTORY:{'|'.join(user_profile['history'])}>\"\n",
    "    prompt = f\"Summarize the following document for a user with preferences: {profile_str}\\n\\nDocument:\\n{document}\\n\\nSummary:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    gen_config = GenerationConfig(\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, generation_config=gen_config)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # post-process: strip prompt prefix\n",
    "    summary_text = text.split(\"Summary:\")[-1].strip()\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a568d-39c5-4b0c-9e81-c523889a4eaf",
   "metadata": {},
   "source": [
    "# CELL - 14: Creating a Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca535eaf-1e28-4faf-b3ef-09f30ff4f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"\"\"Nearly 2,000 miners across four states may lose their jobs after yet another major coal company filed for bankruptcy this week \n",
    "the third since May and fourth since last October. The bankruptcy filing from Revelation Energy LLC and its affiliate Blackjewel LLC, \n",
    "the nation's sixth-top coal producing company in 2017, comes amid President Donald Trump's ongoing efforts to boost the flagging industry. \n",
    "The Trump administration rolled out a rule last month aiming to extend the lives of aging coal-fired power plants across the nation. \n",
    "Environmentalists say the Affordable Clean Energy rule would trigger premature deaths, including from lung disease. During his 2016 \n",
    "presidential campaign, Trump promised to revitalize coal and save miners' jobs, despite scientists linking the burning of the fossil fuels \n",
    "to global warming , but the industry has continued to suffer losses. Coal comeback? Trump plan breathes new life into aging power plants, \n",
    "but critics say climate will suffer At mines and facilities in Virginia, Kentucky, West Virginia and Wyoming, Revelation Energy and \n",
    "Blackjewel employ 1,800 workers, according to court documents and The Casper (Wyo.) Star-Tribune . Company officials estimate they owe \n",
    "$156 million for goods and services, West Virginia Public Radio reported. Last month, Cambrian Coal LLC also filed for bankruptcy . \n",
    "The company operating in Kentucky and Virginia blamed its bankruptcy on changes in demand and regulations related to the Clean Air Act. \n",
    "Another coal-producing leader filed for Chapter 11 bankruptcy in May. Once the nation's third-largest coal company, Wyoming-based Cloud Peak Energy \n",
    "employed 1,300 people at the time of its filing. It accounted for 7.4% of total U.S. coal production in 2017, according to the Department of Labor . \n",
    "And, the nation's ninth-leading coal company went to bankruptcy court late in 2018. Colorado-based Westmoreland Coal Co. had more than $1.4 \n",
    "billion in debt at the time, The Associated Press reported. Gone by 2030?: On World Environment Day, everything you know about energy in \n",
    "the US might be wrong Although Trump has touted coal's rebirth, 51 coal plants have closed and eight coal companies have filed for bankruptcy \n",
    "since his election, CBS News reported last month. Coal's share of the U.S. electricity mix fell from 48% in 2008 to 27% in 2018 and is \n",
    "projected to be 22% in 2020, according to the Department of Energy. \\\"We're retiring a coal plant every month. Coal will all be gone by 2030,\n",
    "said Bruce Nilles , a managing director at the Rocky Mountain Institute, a think tank in Colorado that focuses on energy and resource efficiency. \n",
    "Coal policy, including Trump's Affordable Clean Energy rule , could influence the 2020 election in swing states where coal is still mined, \n",
    "such as Ohio and Pennsylvania. Contributing: Beth Weise and Ledyard King, USA TODAY This article originally appeared on USA TODAY: \n",
    "Is President Donald Trump losing his fight to save coal? Third major company since May files for bankruptcy. \n",
    "\"\"\"\n",
    "\n",
    "sample_doc[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346aa0a-b141-4cb8-a0fe-98d1c88a9989",
   "metadata": {},
   "source": [
    "# CELL - 15: Creating A User Profile for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09320c-4021-440e-85ed-80161379e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a demo user profile based on the sample_doc content\n",
    "demo_profile = {\n",
    "    \"focus\": \"results\",      # focus on the outcome of the events: job losses, bankruptcies\n",
    "    \"tone\": \"analytical\",    # factual, professional summary style\n",
    "    \"length\": \"medium\",      # medium length to capture key points\n",
    "    \"history\": [\n",
    "        \"Coal industry layoffs in Appalachia\",\n",
    "        \"Bankruptcy filings of major mining companies\",\n",
    "        \"Trump administration energy policies\",\n",
    "        \"Impact of Clean Air Act on coal sector\",\n",
    "        \"US electricity mix and coal decline\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Demo user profile:\\n\", demo_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23debc03-eead-4278-887b-0d1bd304eaa7",
   "metadata": {},
   "source": [
    "# CELL - 16: Running the generate_summary function on the Sample Document and User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c6ebd-83a6-4777-aadb-710fd4b71aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_profile = {\"focus\":\"results\",\"tone\":\"analytical\",\"length\":\"short\",\"history\":[\"previous_article_title_1\"]}\n",
    "print(\"Generated summary:\\n\", generate_summary(sample_doc, demo_profile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614fd45-f7f0-4723-a2ba-2fd7bbfdaa5a",
   "metadata": {},
   "source": [
    "# CELL 17: Save artifacts and small README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee8c31-b981-4cfc-ad58-0de0aabdc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"README_MODEL_ARTIFACTS.md\", \"w\") as f:\n",
    "    f.write(\"Artifacts: LoRA weights, tokenizer (if needed), FAISS index (docs_index.faiss).\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
