{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee10dc3-6df9-451d-96c5-ced15dabee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Packages\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa21e1-3c0e-48f7-abf3-270cd17576a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name on Hugging Face\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Your exact BitsAndBytes configuration for 4-bit QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # Recommended for newer GPUs (A100, H100) or high-end consumer cards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d13269-f83c-4f30-91ec-1aa353ab65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "new_model = \"llama-2-7b-summarization-qlora\"\n",
    "learning_rate = 2e-4\n",
    "batch_size = 4\n",
    "num_train_epochs = 3\n",
    "max_seq_length = 1024 # Max sequence length for data (Llama 2 supports 4096, but 1024 is safer for initial fine-tuning)\n",
    "output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b707b0-1be8-4ac0-bb55-06b336a43fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"C:\\Users\\Webbies\\Jupyter_Notebooks\\Assessli_LBM\\Modified_Data.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"[DEBUG] JSON Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d64840-b5a9-400b-b334-15facec4ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert our raw data into Llama Prompt\n",
    "def format_data_to_llama_prompt(example):\n",
    "    \"\"\"Formats each data point into the Llama 2 Chat Instruction format.\"\"\"\n",
    "    \n",
    "    # Extract data fields\n",
    "    profile = example['user_profile']\n",
    "    document = example['document_text']\n",
    "    summary = example['user_generated_summary']\n",
    "\n",
    "    # Create the Instruction part\n",
    "    instruction = f\"\"\"You are an expert summarization model. Summarize the following document based on the user's preferences.\n",
    "    User Preferences:\n",
    "        - Tone: {profile['tone_preference']}\n",
    "        - Length: {profile['summary_length_preference']}\n",
    "        - Focus: {profile['focus_area']}\n",
    "\n",
    "    Document:\n",
    "        {document}\"\"\"\n",
    "\n",
    "    # Format into the Llama 2 Chat template: <s>[INST] Instruction [/INST] Response </s>\n",
    "    # The template is crucial for Llama 2 instruction tuning.\n",
    "    formatted_prompt = f\"<s>[INST] {instruction.strip()} [/INST] {summary.strip()} </s>\"\n",
    "    \n",
    "    return {\"text\": formatted_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf6d0a-889b-4abf-8f9f-8c9d151ca7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dicts to Hugging Face Dataset\n",
    "raw_dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa6b65-d3b3-4093-813d-dea00e80b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function\n",
    "dataset = raw_dataset.map(format_data_to_llama_prompt, remove_columns=raw_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e1814-bd3c-4b5c-9e41-aca2faff41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Llama 2 model with the 4-bit quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Auto-distribute across available GPUs\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 # Set to 1 for 7b models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4155dd5-c6b2-41af-ae0f-0efb37e35441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Llama 2 uses EOS as pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00cea2-20c5-4b4a-b9e5-0c8b8a7281e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,          # Scaling factor for LORA weights\n",
    "    lora_dropout=0.1,       # Dropout probability\n",
    "    r=64,                   # LORA attention dimension (rank)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # Target all Linear layers for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f303098-9895-47fd-be1f-86461971fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\", # Optimized for QLoRA\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True, # Use bfloat16 for faster training (matches bnb_4bit_compute_dtype)\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\" # Remove if you use a logger like wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eba5c-c579-46fd-b8d7-b1eb76ecc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Supervised Fine-Tuning Trainer (SFTTrainer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\", # The column containing the formatted Llama prompt\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23409b4-a1db-4d9e-94e1-a4e62d87ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "print(\"Starting Llama 2 QLoRA fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0a22a-c583-4350-8b8d-4072fbf17c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model (LoRA adapters)\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "print(f\"Fine-tuning complete. LoRA adapters saved to: {new_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
